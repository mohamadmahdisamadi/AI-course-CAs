{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from scipy.stats import multivariate_normal\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [ d for d in range(10) ]\n",
    "names = [\"george\", \"jackson\", \"lucas\", \"nicolas\", \"theo\", \"yweweler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed data.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "### Q1. what are states and observations in the given task?\n",
    "states represent the hidden sequence of discrete values that model can be in at each time. Observations are the visible outputs generated given the hidden states. In this task observations are the MFCC frames and states are the way different speakers pronounces different numbers.\n",
    "### Q2. First-order HMMs.\n",
    "It's called first-order because future states are conditionally independent of the past states given the present state.\n",
    "### Q3. In which contents HMMs are useful and why?\n",
    "tasks that involve sequential data or time-series analysis, especially when the underlying process is not directly observable. because model is based on states and observations. For example speech recognition, NLP, handwriting recognition, predictions over time and music analysis.\n",
    "### Q4.\n",
    "**Pros:**\n",
    "1. Efficient handling of sequential data.\n",
    "2. Flexibility in modeling various scenarios and processes with hidden states.\n",
    "3. Ability to handle missing or noisy data.\n",
    "\n",
    "**Cons:**\n",
    "1. it finds a local extermum and not the global one. meaning that there is no guarantee it leads us to the optimal answer but the answer is often reasonable enough.\n",
    "2. assumptions may lead to loosing important scenarios.\n",
    "3. Limited in modeling long-term dependencies without modifications or extensions.\n",
    "### Q5. different types of HMMs:\n",
    "1. First-order: explained before.\n",
    "2. Fully-connected: all the states are connected to one another and it's more complicated than our model here.\n",
    "3. Continuous-time: state transitions and observations can occur at any time in a continuous time space.\n",
    "4. Hierarchical: These models have multiple levels of hidden states with hierarchical relationships. Useful for modeling complex data with multiple levels of abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.num_hidden_states = num_hidden_states\n",
    "        self.rand_state = np.random.RandomState(1)\n",
    "\n",
    "        self.initial_prob = self._normalize(self.rand_state.rand(self.num_hidden_states, 1))\n",
    "        self.transition_matrix = self._stochasticize(self.rand_state.rand(self.num_hidden_states, self.num_hidden_states))\n",
    "        \n",
    "        self.mean = None\n",
    "        self.covariances = None\n",
    "        self.num_dimensions = None\n",
    "\n",
    "    def _forward(self, observation_matrix):\n",
    "        log_likelihood = 0.\n",
    "        T = observation_matrix.shape[1]\n",
    "        alpha = np.zeros(observation_matrix.shape)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                alpha[:, t] = self.initial_prob[:, t] * observation_matrix[:, t]\n",
    "\n",
    "            else:        \n",
    "                for s in range(self.num_hidden_states):\n",
    "                    if observation_matrix[s][t] == 0: # check if it's possible to observe 'O-t' given 's\"\n",
    "                        alpha[s][t] = 0; continue\n",
    "                    \n",
    "                    sum = 0\n",
    "                    for adj in range(self.num_hidden_states):\n",
    "                        if self.transition_matrix[adj][s]: # check if it's possible to get to 's' from 'adj'\n",
    "                            sum += alpha[adj][t-1] * self.transition_matrix[adj][s]\n",
    "                    alpha[s][t] = sum * observation_matrix[s][t]\n",
    "\n",
    "            alpha_sum = np.sum(alpha[:, t])\n",
    "            alpha[:, t] /= alpha_sum\n",
    "            log_likelihood += np.log(alpha_sum)\n",
    "\n",
    "        return log_likelihood, alpha\n",
    "    \n",
    "\n",
    "    def _backward(self, observation_matrix):\n",
    "        T = observation_matrix.shape[1]\n",
    "        beta = np.zeros(observation_matrix.shape)\n",
    "\n",
    "        beta[:, -1] = np.ones(observation_matrix.shape[0])\n",
    "\n",
    "        for t in reversed(range(T - 1)):\n",
    "            for s in range(self.num_hidden_states):\n",
    "                sum = 0\n",
    "                for adj in range(self.num_hidden_states):\n",
    "                    sum += beta[adj][t+1] * self.transition_matrix[adj][s] * observation_matrix[adj][t+1]\n",
    "                beta[s][t] = sum\n",
    "                    \n",
    "            beta[:, t] /= np.sum(beta[:, t])\n",
    "\n",
    "        return beta\n",
    "    \n",
    "\n",
    "    def _state_likelihood(self, obs):\n",
    "        obs = np.atleast_2d(obs)\n",
    "        B = np.zeros((self.num_hidden_states, obs.shape[1]))\n",
    "\n",
    "        for s in range(self.num_hidden_states):\n",
    "            np.random.seed(self.rand_state.randint(1))\n",
    "            B[s, :] = multivariate_normal.pdf(obs.T, mean=self.mean[:, s], cov=self.covariances[:, :, s])\n",
    "\n",
    "        return B\n",
    "    \n",
    "\n",
    "    def _normalize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x)\n",
    "\n",
    "\n",
    "    def _stochasticize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x, axis=1)\n",
    "\n",
    "\n",
    "    def _em_init(self, obs):\n",
    "        if self.num_dimensions is None:\n",
    "            self.num_dimensions = obs.shape[0]\n",
    "        if self.mean is None:\n",
    "            subset = self.rand_state.choice(np.arange(self.num_dimensions), size=self.num_hidden_states, replace=False)\n",
    "            self.mean = obs[:, subset]\n",
    "        if self.covariances is None:\n",
    "            self.covariances = np.zeros((self.num_dimensions, self.num_dimensions, self.num_hidden_states))\n",
    "            self.covariances += np.diag(np.diag(np.cov(obs)))[:, :, None]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _em_step(self, obs):\n",
    "        obs = np.atleast_2d(obs)\n",
    "        T = obs.shape[1]\n",
    "\n",
    "        B = self._state_likelihood(obs=obs)\n",
    "\n",
    "        log_likelihood, alpha = self._forward(observation_matrix=B)\n",
    "        beta = self._backward(observation_matrix=B)\n",
    "\n",
    "        xi_sum = np.zeros((self.num_hidden_states, self.num_hidden_states))\n",
    "        gamma = np.zeros((self.num_hidden_states, T))\n",
    "\n",
    "        for t in range(T-1):\n",
    "            partial_sum = alpha[:,t] @ ((beta[:,t+1] * B[:,t+1]).T * self.transition_matrix)\n",
    "            xi_sum += self._normalize(partial_sum)\n",
    "        \n",
    "        for t in range(T):\n",
    "            gamma[:, t] = self._normalize(alpha[:, t] * beta[:, t])\n",
    "        \n",
    "        expected_prior = gamma[:, 0].reshape(-1, 1)\n",
    "        expected_transition = self._stochasticize(xi_sum)\n",
    "        \n",
    "        expected_covariances = np.zeros((self.num_dimensions, self.num_dimensions, self.num_hidden_states))\n",
    "        expected_covariances += .01 * np.eye(self.num_dimensions)[:, :, None]\n",
    "\n",
    "        gamma_state_sum = np.sum(gamma, axis=1)\n",
    "        gamma_state_sum = gamma_state_sum + (gamma_state_sum == 0)\n",
    "\n",
    "        expected_mean = np.zeros((self.num_dimensions, self.num_hidden_states))\n",
    "        for s in range(self.num_hidden_states):\n",
    "            gamma_obs = obs * gamma[s, :]\n",
    "            expected_mean[:, s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]\n",
    "\n",
    "        self.initial_prob = expected_prior\n",
    "        self.mean = expected_mean\n",
    "        self.transition_matrix = expected_transition\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "    def train(self, obs, num_iterations=1):\n",
    "        for i in range(num_iterations):\n",
    "            self._em_init(obs)\n",
    "            self._em_step(obs)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def score(self, obs):\n",
    "        B = self._state_likelihood(obs)\n",
    "        log_likelihood, _ = self._forward(B)\n",
    "        return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_SAMPLES = 40\n",
    "NUM_STATES = 13\n",
    "NUM_ITER = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(col, value, implementing_method):\n",
    "    df_value = df[df[col] == value]\n",
    "    df_value = df_value[df_value[\"repeat num\"] < NUM_TRAIN_SAMPLES]\n",
    "    df_value = df_value.reset_index()\n",
    "    mfccs = [df_value[\"MFCCs\"][i] for i in range(len(df_value))]\n",
    "    obs = np.concatenate(mfccs, axis=1)\n",
    "    \n",
    "    if implementing_method == \"library\":\n",
    "        lengths = [mfcc.shape[1] for mfcc in mfccs]\n",
    "        hmm_model = hmm.GaussianHMM(n_components=NUM_STATES)\n",
    "        hmm_model.fit(obs.T, lengths=lengths)\n",
    "        \n",
    "    elif implementing_method == \"scratch\":\n",
    "        hmm_model = HMM(num_hidden_states=NUM_STATES)\n",
    "        hmm_model = hmm_model.train(obs=obs, num_iterations=NUM_ITER)\n",
    "        \n",
    "    return hmm_model\n",
    "        \n",
    "def train_models(col, values, method):\n",
    "    hmm_models = dict()\n",
    "    for value in values:\n",
    "        hmm_models[value] = train_model(col=col, value=value, implementing_method=method)\n",
    "    return hmm_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_test(col, value):\n",
    "    return df[(df[col] == value) & (df[\"repeat num\"] >= NUM_TRAIN_SAMPLES)].reset_index()\n",
    "\n",
    "\n",
    "detected_d_l, detected_d_s = [[0 for _ in digits] for _ in range(2)]\n",
    "detected_s_l, detected_s_s = [[0 for _ in names] for _ in range(2)]\n",
    "def test_models(models_l, models_s, col, values, value):\n",
    "    df_test = create_df_test(col=col, value=value)\n",
    "\n",
    "    passed_l, passed_s = 0, 0\n",
    "\n",
    "    for i in range(len(df_test)):\n",
    "        obs = df_test[\"MFCCs\"][i]\n",
    "        scores_l = [ [models_l[v].score(obs.T) for v in values] ]\n",
    "        scores_s = [ [models_s[v].score(obs) for v in values] ]\n",
    "\n",
    "        if values[np.argmax(scores_l)] == value:\n",
    "            passed_l += 1\n",
    "        if values[np.argmax(scores_s)] == value:\n",
    "            passed_s += 1\n",
    "        if col == \"digit\":\n",
    "            detected_d_l[np.argmax(scores_l)] += 1\n",
    "            detected_d_s[np.argmax(scores_s)] += 1\n",
    "        elif col == \"speaker\":\n",
    "            detected_s_l[np.argmax(scores_l)] += 1\n",
    "            detected_s_s[np.argmax(scores_s)] += 1\n",
    "\n",
    "    failed_l, failed_s = len(df_test) - passed_l, len(df_test) - passed_s\n",
    "    \n",
    "    return [passed_l, failed_l, passed_s, failed_s, len(df_test)]\n",
    "\n",
    "\n",
    "def run_tests(models_l, models_s, col, values):\n",
    "    result = dict()\n",
    "    for value in values:\n",
    "        result[value] = test_models(models_l=models_l, models_s=models_s, col=col, values=values, value=value)\n",
    "    return result\n",
    "\n",
    "\n",
    "def show_result(result):\n",
    "    df_res = pd.DataFrame(data=result)\n",
    "    df_res = df_res.transpose()\n",
    "    cols = {0: \"passed (l)\", 1: \"failed (l)\",\n",
    "            2: \"passed (s)\", 3: \"failed (s)\",\n",
    "            4: \"tests count\"}\n",
    "    df_res.rename(columns=cols, inplace = True)\n",
    "    for col in df_res.columns:\n",
    "        if \"passed\" in col or \"failed\" in col or \"count\" in col:\n",
    "            df_res[col] = df_res[col].astype(int)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_model_l = train_models(col=\"digit\", values=digits, method=\"library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_model_s = train_models(col=\"digit\", values=digits, method=\"scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model_l = train_models(col=\"speaker\", values=names, method=\"library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model_s = train_models(col=\"speaker\", values=names, method=\"scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_digit = run_tests(models_s=digit_model_s, models_l=digit_model_l, col=\"digit\", values=digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_speaker = run_tests(models_s=speaker_model_s, models_l=speaker_model_l, col=\"speaker\", values=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_d = show_result(result=result_digit)\n",
    "df_res_d[\"detected (l)\"] = detected_d_l\n",
    "df_res_d[\"detected (s)\"] = detected_d_s\n",
    "\n",
    "df_res_s = show_result(result=result_speaker)\n",
    "df_res_s[\"detected (l)\"] = detected_s_l\n",
    "df_res_s[\"detected (s)\"] = detected_s_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "Helps us to save time in the future step.s Also provides transfering data to other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result speaker.pkl', 'wb') as file:\n",
    "    pickle.dump(df_res_s, file)\n",
    "with open('result digit.pkl', 'wb') as file:\n",
    "    pickle.dump(df_res_d, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
